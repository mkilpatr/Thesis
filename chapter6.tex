\chapter{Uncertainties and Estimation}
\label{ch:UncertandEst}

We now have a robust method for the prediction of SM background for our search for the top squark. Along with a prediction for each of the backgrounds, we need to include the systematics uncertainties for our methods. Once we incorporate them we can confirm that our methods are correct by looking at the validation regions. Finally, we can then compare the final comparisons of data-to-simulation in our SR, which will allow us to set limits on the top squark mass. 

\section{Systematic Uncertainties}\label{sec:Uncert}
We will look at the various categories of systematic uncertainties that can affect the analysis. 
\begin{itemize}
	\item \textbf{Statistical uncertainty of control regions in data}: The dominant uncertainty in data-drivel background prediction methods is generally the result of limited statistics of the control regions that are used to derive the background estimate for the signal regions.
	\item \textbf{Statistical uncertainty of simulated samples}: In some cases, the uncertainties on transfer factors derived from simulation also have a significant statistical component due to limited statistics in the simulated samples.
	\item \textbf{Uncertainties related to extrapolation from control to signal regions}: Each data-driven background estimation method relies on an extrapolation from a control region to the signal region. This leads to uncertainties specific to each method, related to the nature of the extrapolation. For instance, the LL prediction strategy uses the region with a selected lepton to estimate the background yield in the vetoes region. Appropriate correction factors are applied to both selected and vetoes regions to account for differences in the lepton selection efficiency between data and simulation. The precision of these corrections affects the uncertainty in the background estimation. For the QCD prediction, we rely on an extrapolation from the low $\Delta\phi_{12}$ region, which is mostly in the core of the jet response distribution, to a region of high values of $\Delta\phi_{12}$, which is mostly in the tail of the distribution. We therefore assign an uncertainty in the QCD precition for the potential effects of severe jet mismeasurements in the jet response tails. 
	\item \textbf{Uncertainties related to $b$-tagging}: Effects related to the $b$-tagging are estimated to impact the scale factors up to 3\% for top-jets and between 5 to 10\% for \W+jets. Uncertainties due to the top-\pt{} reweighting, pile-up, or matching criteria results to effect smaller than 3\% for both top and \W{} tags. 
	\item \textbf{Uncertainties related to soft $b$-tagging}: The soft $b$-tagging methods are dependent on low-\pt{} jets and track reconstruction. The impact of these uncertainties are negligable for \ttbar{} and \W+jets and around 5\% for QCD events.
	\item \textbf{Uncertainties related to the merged top and W-tagging}: The impact of different effects on the determination of the scale factors are studied. One of the dominant sources is related to the description of the parton showering which results to uncertainties of 5-25\% and 8-15\%, for top and \W{} tagging, respectively. Another source of systematics is due to the modeling of the \ttbar{} topology. The evaluated uncertainties range between 1-3\% (1-12\%) in top (\W) tagging. 
	\item \textbf{Uncertainties related to the Resolved Top Tagging}: A similar list of the sources of systematic uncertainties is evaluated for the resolved tagger, as in the case of Merged tops. The most important sources stems from the description of the parton showering, which ranges between 8-31\%. The modeling of the \ttbar{} topology can be as high as 4\%, whereas potential dependence on the $b$-tagging results to uncertainties from 1 to 6\%. Uncertainties due to the top-\pt{} reweighting, pile-up, or matching criteria results to effect smaller than 3\%. 
	\item \textbf{Uncertainties related to lepton identification}: The control region of the LL is defined as having a single muon or electron. We must account for the acceptance and scale factor corrections for each particle. The impact is an approx. 1\%  and <1\% uncertainty for electrons and muons, respectively. It is also necessary to account for the misidentification of leptons (electrons, muons, and taus) that can contaminate our search region. 
	\begin{equation}\label{eqn:lepvetosyst}
		SF_{l}=1+\frac{N_l^{yield}-N_l^{SF}}{N_{SR}}
	\end{equation}
	This is shown in Eqn. \ref{eqn:lepvetosyst} where $N_l^{yield}$ and $N_l^{SF}$ are  the yield of simulation in the search region with a requirement of $N_l\geq1$ with and without the lepton scale factor and $N_{SR}$ is the yield in the search region with the lepton veto.  
	%\item \textbf{Luminosity}: A 2.6\% uncertainty is assigned on the integrated luminosity measured by CMS for the 2016 data-taking period. The prediction of the diboson backgrounds, which rely o simulaiton and are normalized based on the measured integrated luminosity, are affected by this uncertainty. Since the estimations of the remaining SM background processes are data-driven and based on the data observed in control regions, they are not affected by this uncertainty. 
\end{itemize}

\section{Validation}\label{sec:Validation}

\input{tables/validationregions.tex}

In order to test and validate the background estimation strategy in data, we carry out the background estimation method in a lower \met{} region of the zero-lepton sample that is adjacent to the search sample, "low \met{} validation sample", and check the agreement between data and background prediction. The validaiton sample has significantly larger statistics than the search sample and is signal-depleted. Apart from the difference in the \met{} selection, the search selection on the other search variables is applied to the validation sample, with an exception of the regions with more than one top- or W-tags, where relaxed selections (i.e. drop selection in $M_T(b_{1,2},\met)$) are applied to gain more statistics, see Tables \ref{tab:validationregions-hm} and \ref{tab:validationregions-lm}. Figures \ref{fig:validation-region-lm} and \ref{fig:validation-region-hm} displays the SM estimate and the observed data in the different validation regions. Statistical uncertainties as well as systematic uncertainties resulting from the top- and W-tagging correction in the background predictions are shown in this plot. The data agrees well with the estimated backgrounds yields within uncertainties. 

\begin{figure}
	\begin{center}
  \includegraphics[width=1\textwidth]{validation_lowdm.png}
	\end{center}
	\caption[LM Validation Region]{Comparison of the data and SM backgrounds in the Low \dm{} validation regions.
	 }
	\label{fig:validation-region-lm}
\end{figure}

\begin{figure}
	\begin{center}
  \includegraphics[width=1\textwidth]{validation_highdm.png}
	\end{center}
	\caption[Lost Lepton HM Control Region]{Comparison of the data and SM backgrounds in the High \dm{} validation regions.
	 }
	\label{fig:validation-region-hm}
\end{figure}

Now that we have shown that the bins for the low and high \dm{} match well with data we can move to a full comparison of the data and simulation in the SR. To do this we combine the predictions from the LL, \Znunu, QCD, and Rare backgrounds. We need to 

\section{Combination of Search region}\label{sec:Combination}

As we have seen, this analysis has quite a large parameter space of interesting mass points. This combined with the number of SR bins and different backgrounds in each bin it is necessary to discuss how we plan of combining all of this to get a final estimation. To do this we use a datacard method to perform the combination and uncertainty calculation for our counting experiment. In Fig. \ref{fig:example-datacard-sig}, we have an example of a single bin from our SR, specifically bin 61 (high $\dm, \nb=1, \mtb\geq175, \nt=0, \nrt=0, \nw=0, \Ht\geq1000, 250\leq\met<350$), for a single signal sample, T2tt(175, 1). 

Let go through Fig. \ref{fig:example-datacard-sig}, so that we can understand what is going on. The parameter \textit{imax} is the number of final states analyzed in the datacard, \textit{jmax} is the number of independent physical processes whose yields are provided minus 1, and \textit{kmax} is the number of independent systematic uncertainties. Here we put "*" and allow the computation to figure that out. Next, we have the number of observed events from data in this particular bin, shown as \textit{observation}, where the bin above it is a unique name. Then, in the five lines following, we have the yields for the different processes included in this bin. The first process denoted as \textit{signal} is the yield from T2tt(175,1), then the other four processes are the SM backgrounds in this bin. 

\begin{figure}
	\begin{center}
  \includegraphics[width=1\textwidth]{T2tt_175_1_example_bin_hm_nb1_highmtb_nt0_nrt0_nw0_htgt1000_MET_pt250to350.png}
	\end{center}
	\caption[Example Datacard of T2tt(175,1)]{An example of the datacard for a single bin of T2tt(175,1) with background and uncertainties.
	 }
	\label{fig:example-datacard-sig}
\end{figure}
\begin{figure}
	\begin{center}
  \includegraphics[width=1\textwidth]{bin_lepcr_hm_nb1_highmtb_ht1000to1300_MET_pt250to350.png}
	\end{center}
	\caption[Example Datacard of LL CR]{An example of the datacard for the LL CR with signal contamination.
	 }
	\label{fig:example-datacard-ll}
\end{figure}
\begin{figure}
	\begin{center}
  \includegraphics[width=1\textwidth]{bin_qcdcr_hm_nb1_highmtb_ht1000to1300_MET_pt250to350.png}
	\end{center}
	\caption[Example Datacard of QCD CR]{An example of the datacard for the QCD CR.
	 }
	\label{fig:example-datacard-qcd}
\end{figure}

The next major portion of the datacard is the various systematics of the signal and background processes. The most common type of uncertainty is the log-normal which is denoted by \textit{lnN}. For these, the distribution is described by the parameter $\kappa$, which is a multiplicative error. A $+1\sigma$ deviation corresponds to a yield scaled by $\kappa$ while a $-1\sigma$ scales the yields by $\frac{1}{\kappa}$. If the uncertainties are small the log-normal error can be approximated by a gaussian and we can approximate $\kappa=1+\frac{\delta x}{x}$, where $\frac{\delta x}{x}$ is the relative uncertainty of the event yield. Each process has a value for the statistical uncertainty which is a poisson error of $\sqrt{N}$. Then each process can have a systematic error that may be correlated to other backgrounds. In each process, we can have a value for the uncertainty or a dash which means that the uncertainty does not contribute to the process. 

Finally at the bottom of the datacard we have the rate parameters for the backgrounds that require the extrapolation from units to SR bins. The units are a more basic definition of the SR bins that can be combined with the prediction of each background to give the total background in each SR bin. Since the LL, QCD, and \Znunu{} backgrounds use an extrapolation method for the high \dm{} bins, we have defined 112 CR units and 529 SR units. In the bottom of Fig. \ref{fig:example-datacard-sig}, are the expected rates for the LL, QCD, and \Znunu{} that show the combination of three units to give the prediction of each background in the respective bins. The rates from the LL and QCD background can be seen in Fig. \ref{fig:example-datacard-ll} and \ref{fig:example-datacard-qcd}. These are an example of a single unit datacard. The calculation for each background from the unit CR is the data observation divided by the SM background in each, $\frac{N_{data}^{CR}}{(N_{LL}^{CR} + N_{Sig}^{CR})}$ where the LL CR is used as an example. Now to be able to make the estimation of the limit for the various SUSY processes that we are looking at, we need to combine the datacards for each SR bin for all of the signals in the mass parameter space. This can be easily accomplished thanks to the unique bin and process names for each signal. We can then input each of these combined datacards into the Higgs Combined Tool.

\section{Statistics}\label{sec:Statistics}

For the combination of multiple analyses that are sensitive to different signal production mechanisms and different decay modes, we are required to set limits on a common signal strength modifier $\mu$ that is used to alter the cross sections of all the production mechanisms. It is conventional to require a 95\% confidence level (CL) to exlude a signal. The interpretation of the CL, however, is quite complicated. We use a modified classical frequentist method such that we can include systematic uncertainties in the results. The method for constructing a unified confidence interval is based on the likelihood-ratio test statistic \cite{feldman_unified_1998}, 
\begin{equation}\label{eqn:likelihoodtest}
q_\mu=-2\text{ln}\frac{\mathcal{L}(data|\mu s+b)}{\mathcal{L}(data|\hat{\mu} s+b)},
\end{equation}
where $\hat{\mu}$ is used to maximize the likelihood $\mathcal{L}(data|\mu s+b)$, and $s$ and $b$ are the signal and background, respectively. In order to add systematics on the signal and background rates, $s(\theta)$ and $b(\theta)$, we include a prior pdf for the nuisance $\theta$ as $\rho(\theta|\widetilde{\theta})$ where $\widetilde{\theta}$ is the nominal value of the nuisance parameter. We interpret the systematic uncertainty as posteriors of the measurements. 

\subsection{Profile Likelihood Asymptotic Approximation}\label{subsec:Asymptotic}

The profile likelihood ratio is shown in Eqn. \ref{eqn:likelihoodtest} with the addition of the constraint $0\leq\widehat{\mu}\leq\mu$ which ensures that the obtained limits are one-sided. Additionally, this can be approximated by an asymptotic formula which is based on the Wilks and Wald theorems \cite{cowan_asymptotic_2011}. Since we include the physical constraint $\widehat{\mu}>0$, the asymptotic features of $f(\widetilde{q}_\mu|\text{signal+background})$, where $f$ is the \textit{pdf} of $q_\mu$ assuming the hypothesis $\mu$, is provided by the well defined formula \cite{cowan_asymptotic_2011}, 
\begin{equation}\label{eqn:profileLikelihood}
f(\widetilde{q}_\mu|\mu)=\frac{1}{2}\delta(\widetilde{q}_\mu)+\left\{
\begin{split}
&\frac{1}{2}\frac{1}{\sqrt{2\pi}}\frac{1}{\sqrt{\widetilde{q}_\mu}}e^{-\widetilde{q}_\mu/2} & 0<\widetilde{q}_\mu\leq\mu^2/\sigma^2 \\
&\frac{1}{\sqrt{2\pi}(2\mu/\sigma)}exp[-\frac{1}{2}\frac{(\widetilde{q}_\mu+\mu^2/\sigma^2)^2}{(2\mu/\sigma)^2}] & \widetilde{q}_\mu>\mu^2/\sigma^2
\end{split}
\end{equation}
where $\sigma^2=\frac{\mu^2}{q_{\mu,A}}$ with $q_{\mu,A}$ is evaluated with the expected background and nominal nuisance parameters. 

\section{Higgs Combined Tool}\label{sec:HiggsCombined}

The Higgs Combined Tool \cite{noauthor_procedure_2011} uses a modified frequentist method to compute the CL \cite{read_presentation_2002, junk_confidence_1999, read_modified_2000} for each of the cross section limits. The tool is able to convert the datacards into a Likelihood function to calculate the limits. The Likelihood function is 
\begin{equation}\label{eqn:likelihood}
\mathcal{L}(\mu,\theta)=\prod_i \frac{[\mu\cdot s_i(\theta)+b_i(\theta)]^{n_i}}{n_i !}e^{-[\mu\cdot s_i(\theta)+b_i(\theta)]}\prod_\kappa e^{-\frac{1}{2}\theta^2_\kappa},
\end{equation}
where $\mu$ is the POI which is constrained only by the observed data, $s_i(\theta)$ and $b_i(\theta)$ are nuisance parameters for signal and background. Both the signal and background are subject to multiple uncertainties that are dealt with by using nuisance parameters $\theta$. Each of these three parameters are jointly fitted to get a value of $\mu$. We will be using an asymptotic CL method to compute the limits for this analysis. 

\subsection{Observed Limits}\label{sec:ObsLimits}

To compute the observed limit the likelihood function, $\mathcal{L}(\mu,\theta)$ from Eqn. \ref{eqn:likelihood}. Comparing the compatability of the data with the background-only and signal+background hypotheses to get 
\begin{equation}
\widetilde{q}_\mu=-2 ln\frac{\mathcal{L}(\mu,\hat{\theta}_\mu)}{\mathcal{L}(\mu,\theta)},
\end{equation}
with a constraint $0\leq\hat{\mu}\leq\mu$ where $\hat{\theta}_\mu$ is the conditional maximum likelihood estimators of $\theta$ given the signal strength $\mu$ and data, and the estimators $\hat{\mu}$ and $\hat{\theta}$ are the global maximum of the likelihood. 

From this, we get the observed value of $\widetilde{q}_\mu^{obs}$ for a given value of $\mu$. A toy Monte Carlo pseudo-data is used to construct the PDFs $f(\widetilde{q}_\mu|\mu,\hat{\theta}_\mu^{obs})$ and $f(\widetilde{q}_\mu|0,\hat{\theta}_0^{obs})$ where we have a signal with strength $\mu$ in the signal+background and a background-only hypothesis. Two $p$-values are defined and associated with the two hypotheses, $p_\mu$ and $p_b$,
\begin{equation}\label{eqn:splusb}
p_\mu=P(\widetilde{q}_\mu\geq\widetilde{q}_\mu^{obs}|\text{signal+background})=\int_{\widetilde{q}_\mu^{obs}}^{\infty}f(\widetilde{q}_\mu|\mu,\hat{\theta}_\mu^{obs})d\widetilde{q}_\mu,
\end{equation} \label{eqn:b}
we then have
\begin{equation}
1-p_b=P(\widetilde{q}_\mu\geq\widetilde{q}_\mu^{obs}|\text{background-only})=\int_{\widetilde{q}_\mu^{obs}}^{\infty}f(\widetilde{q}_\mu|0,\hat{\theta}_0^{obs})d\widetilde{q}_\mu,
\end{equation}
using both of these we can calculate the $CL_s(\mu)$ as 
\begin{equation}
CL_s(\mu)=\frac{p_\mu}{1-p_b}.
\end{equation}
Finally, to quota a $95\%$ confidence level on the upper limit on $\mu$, $\mu$ is adjusted until the $CL_s=0.05$ such that the confidence level is $(1-\alpha)$ where $CL_s\leq\alpha$. 

\subsection{Expected Limits}\label{sec:ExpLimits}

Along with the observed limits we are also interested in calculating the expected median upper-limit, $\pm1\sigma$, and $\pm2\sigma$ bands for the background-only hypothesis. To do this we generate a large set of background-only pseudo-data, using the toy Monte Carlo method, and calculate the $CL_s$ as if it was real data. The calculation for the expected limit is done by finding the cumulative probability distribution function (CDF) of results and starting the integration. The point at which the CDF crosses the 50\% band is the median expected value. Then the $\pm1\sigma$ band is defined by the 16\% and 84\% band and the $\pm2\sigma$ band is the 2.5\% and 97.5\% crossings. 

\section{Results}\label{sec:Results}

\begin{figure}
	\begin{center}
  \includegraphics[width=0.8\textwidth]{limits/T2tt_testXSEC.pdf}
	\end{center}
	\caption[Run 2: T2tt Limits]{Limits for the mass parameter space for $\st\rightarrow t \neutralino$ (T2tt) decays for Run 2 data. With a current limit of 1.4 \TeV{} for a minimal neutralino mass.
	 }
	\label{fig:Run2-T2tt-limits}
\end{figure}

After creating all of the datacards for each mass point we can calculate the expected and observed limits for the entire Run 2 analysis for each signal region, T2tt, T2bW, T2tb, T2fbd, and T2bWC. 
\chapter{Top Squark Production and Backgrounds}
\label{ch:Search}

We have previously motivated that the top squark, \st{}, could be the lightest squark, see Sec. \ref{subsec:StopMassSpec}. This allows us to possibly produce them at CMS. In Sec. \ref{sec:Baseline}, we have designed a search that will look for events with large amounts of \met{} and \nj. These events are being targeted in separate low \dm{} and high \dm{} search regions. For all of these we are interested in comparing the data and background in each search or control region. We will look at the production and decay modes of various \st{} interactions and the estimation of the SM background of each region. 

\section{Production and Decay Modes}
\label{sec:Production}

To produce the top squark we need is the collision of high energy particles, such that there is enough energy available to pair produce them. It is shown as a black circle in Fig. \ref{fig:stop-gluino-production} and \ref{fig:stop-direct-production}. This is meant to represent many processes that can make a top squark. The main processes are gluon fusion, when two gluons fuse into a single gluon which then decays into a top and anti-top squark pair, or annihilation, where two quarks annihilate to a gluon propagator which thus decays into two top squarks. 

\input{StopSUSYModes.tex}

The main decay mode of the top squark is $\st\rightarrow t+\neutralino$ and $\st\rightarrow b+\chargino$, Fig. \ref{fig:stop-direct-production}. The top quark most likely decays as, $t\rightarrow b\W^+$, while the $b$ quark will decay to either a $c$ or an $u$ quark in its decay chain with an additional \W{} boson. The \neutralino{} is proposed to be a stable dark matter candidate while the \chargino{} could decay as, $\chargino\rightarrow\neutralino\W$. Next, a four body decay is allowed for, $\st\rightarrow b f f^\prime \neutralino$, see Fig. \ref{fig:stop-direct-production}(d, e). The final direct \st{} production we are interested in is the $\st\rightarrow c\neutralino$, see Fig. \ref{fig:stop-direct-production}(f). To be as inclusive as possible, we are also including indirect top squark production \cite{cms_collaboration_search_2013} as seen in Fig. \ref{fig:stop-gluino-production}. We see that the \st{} will decay to multiple jets, \nj, and missing transverse energy, \met. Now we are going to try to estimate the SM background that could be in each of our search region bins.

\section{Standard Model Background}
\label{sec:SMBackground}

The standard model background for the top squark search is defined by a large \met{} and multiple jets. The are a couple types of SM background that can be misinterpreted as our signal. The most likely background is that which causes many tops (or heavy particles) and missing energy. Events in the SM like \ttbar{} and \W+jets will have many jets produced and \met{} due to a missed lepton and neutrino. The production of heavy particles like \Znunu{} will give multiple jets and \met{} from the neutrinos being missed by the detector. QCD events often produce events with multiple jets. Due to acceptances in the detector jets can sometimes be mis-measured which can cause large \met{}. There are also various processes that are quite rare but still need to be included. We will take an in-depth look into each of these backgrounds. 

\section{Lost Lepton}
\label{sec:LL}

\begin{figure}
	\begin{center}
  \includegraphics[width=0.9\textwidth]{LL/ttbar_diagrams.png}\\
  \includegraphics[width=0.30\textwidth]{LL/wjets_diagrams.png}
  \includegraphics[width=0.30\textwidth]{LL/TTW_diagrams.png} 
  \includegraphics[width=0.30\textwidth]{LL/tW_diagrams.png} \\

	\caption[\ttbar{} Production]{Production Feynman diagrams for the LL backgrounds \cite{fiedler_precision_nodate, khachatryan_search_2016, aad_measurement_2013}. }
	\label{fig:llb-ttbar-diagram}
	\end{center}
\end{figure}

The contribution from the \ttbar{} and \W+jets processes arise from leptonic decays of the \W{} boson, where the charged lepton is outside the kinematic acceptance of CMS or evades identification by the dedicated lepton vetoes. Large \met{} can be generated by the associated neutrino and the lepton that is not reconstructed, allowing such events to enter the search regions. This background is collectively referred to as the "Lost Lepton" (LL) background. Contributions arising from $tW, ttW$ and single-top processes also enter into this category, but with much smaller importance. 

Studies of simulated events, indicate that the event kinematics for different lepton flavors are similar enough to allow us to estimate them collectively from a single control sample in data that has event characteristics similar to those of the search sample. Because of this, we use the single-lepton control sample to estimate the LL background, using the method described in detail in Ref. \cite{bravo_search_2015}. The single-lepton sample consists of events that have one lepton satisfying the lepton-veto criteria. In order to suppress potential signal contamination, we require $M_T(l,\met)<100~\GeV$. The requirement of low $M_T(l,\met)$ also ensures orthogonality to the search regions used in direct top squark production in the single-lepton or double-lepton final state, making it possible to statistically combine the results of the two searches. The selection applied to the single-lepton control sample follows the same selection on the search variables as in the zero-lepton selection with the exception of classification according to the number of top and \W-tagged candidates. 

\subsection{Combining All Run 2 Eras}\label{sec:LLCombination}
Firstly, each year of data can have a different detector configuration and corresponding simulation. These are split into separate eras that have accurate modeling of the data. We have defined five different eras for the data and simulation, 2016, 2017 RunBtoE, 2017 RunF, 2018 preHEM, and 2018 postHEM. The splits for 2017 are due to different average pileup and 2018 is split to take into account the loss of a part of the HCAL. For this analysis we are interested in the possibility of combining the yields of each era into one estimation. This is initially done by looking at the \met{} distributions in each era. Since the LL estimation is done with the transfer factor method, a good confirmation would be the comparison of the transfer factor in each SR for each era. 
\input{LostLepton_Inclusive_Combination.tex}

\subsection{Transfer Factors}
\label{subsec:TF}

The LL estimation in each search region is based upon the event count in data in the corresponding control region in the single-lepton sample. The count is extrapolated to the search region to obtain a prediction by means of a transfer factor obtained from simulation as follows: 

\begin{equation}
\label{eqn:LLTF}
N_{pred}^{LL}=TF_{LL} \cdot N_{data}(1l).
\end{equation}

This allows us to have the same selection for the single-lepton control sample and the zero-lepton sample. The only exception is the number of top and W-tagged candidates. The LL estimation is dependent on the yield of data in the corresponding CR and the TF calculated by the single-lepton sample. The transfer factor is defined as, 
\begin{equation}
\label{eqn:TF}
TF_{LL}=\frac{N_{MC}(0l)}{N_{MC}(1l)},
\end{equation}
where $N_{MC}(1l)$ is the event count observed in the corresponding CR and $N_{MC}(0l)$ uses the event count in the corresponding SR. 

The main motivation behind this approach is to increase the statistical precision of the background estimation. The performance of the $t$ and \W{} taggers has been studied in data and MC samples and a reasonably good agreement is observed allowing us to proceed with this approach. Data-to-MC scale factors are extracted and applied to MC to account for residual differences of the tagging performance in data. Detailed studies comparing the performance of the $t$ and \W{} taggers in data and MC are found in Ref. \cite{cms_collaboration_search_2016}.

The control regions utilized to predict the LL background are displayed in Figs. \ref{fig:llb-1lcr-datavsmc-lm-nb0} to \ref{fig:llb-1lcr-datavsmc-hm-nb3-1}. Figures \ref{fig:llb-1lcr-datavsmc-lm-nb0} and \ref{fig:llb-1lcr-datavsmc-lm-nb1} display the control regions specific to the low \dm{} selection, where the regions are binned following the search region definition. Figures \ref{fig:llb-1lcr-datavsmc-hm-nt0-nrt0-nw0} to \ref{fig:llb-1lcr-datavsmc-hm-nb3-1} display the control regions dedicated for the high \dm{} selection. Due to the nature of the background estimation method applied in the high \dm{} search, control regions are utilized for the prediction of multiple search regions. 

Tables \ref{tab:0l-llb-pred-lm} to \ref{tab:0l-llb-pred-hm-3} summarize the yields in data observed in the single-lepton sample, the derived transfer factor, and the resulting LL predictions for the low \dm{} and high \dm{} search regions, respectively. The transfer factors in the high \dm{} region actually account for two levels of extrapolation. The CR for the high \dm{} is loose, such that, there is no binning in tops or W tagging. We then extrapolate to the SR with the inclusion of the top and W tags, along with scale factors, to estimate the LL background in the region with:
\begin{equation}\label{TFExtrapolation}
\begin{split}
\tfll=& \tfll^{CR-SR}\times \tfll^{SR-extrap} \\
=& \frac{N_{MC}(0l)(\nj,\nb,\met)}{N_{MC}(1l)(\nj,\nb,\met)}\times\frac{N_{MC}(0l)(\nj,\nb,\met,\nt,\nrt,\nw)}{N_{MC}(0l)(\nj,\nb,\met)}.
\end{split}
\end{equation}

We now want to consider how the transfer factor for each era relates to the total transfer factor. In Figs. \ref{fig:llb-1lcr-datavsmc-total-tf} and \ref{fig:llb-1lcr-datavsmc-sep-tf}, we see the comparison for the total \tfll{} for each era of the data and simulation. These are all in quite good agreement, but we see an extended tail due to the low statistics of the extrapolation in $t$ and \W. Once we alter the comparison for the \tfll{} to separate the CR-to-SR and the SR-to-Extrapolation, we see a much better agreement when for the CR-to-SR comparison. These are improved because of the better statistics in the region. 
\input{LostLepton_TransferFactor_Comparison.tex}
\input{tables/yields_llb_all_lm.tex}
\input{tables/yields_llb_all_hm.tex}
\input{LostLepton_LM_CR_plots.tex}
\input{LostLepton_HM_CR_plots.tex}

Now that we have confirmed that the LL background is valid to be combined for all the eras we can combine the transfer factors and control region comparisons of each era in the final estimation. Now, we can consider the combination of the other background estimations. 

\section{Z Boson Decay to Neutrinos}
\label{sec:Znunu}

An important source of background for the zero-lepton search is from events in which a \Z{} boson decays to neutrinos. Since neutrinos are weakly interacting they are missed by the CMS detector which results in \met. Two methods are traditionally used to estimate the \Znunu{} background. The first method makes use of a sample dominated by \Zll+jets events which has the same kinematics, but has much lower statistics in the tight search regions. The second method utilizes a $\gamma+$jets sample which has a factor of 5 or more larger cross section and similar leading Feynman diagrams. The main differences are quark-boson couplings and the fact that the \Z{} boson is very massive. In the realm of the analysis, where events have large boson \pt{} these effects become less important. The \met{} of the $\gamma+$jets process is calculated after removing the photon from the event to mimic the \Znunu{} process.

We use a hybrid method to estimate the \Znunu{} background that makes use of both the $\gamma+$jets and the \Zll+jets processes. The photon and the dilepton system are removed from the events before calculating \met{} and other kinematic variables related to \met, and the modified \met{} is denoted by \metgamma{} and \metll{} for $\gamma+$jets and the \Zll+jets processes, respectively. The \Zll+jets sample is used to measure the normalization of the \Znunu{} process in different ranges of \nb{} and \nsv, and we take advantage of the much higher statistics of the $\gamma+$jets sample to extract shape corrections. 

\subsection{Prediction Method}\label{subsec:znunupred}

The prediction of the \Znunu{} background is given by:
\begin{equation}
N_{pred}^{\Znunu}=N_{MC}^{\Znunu} \cdot R_Z \cdot S_\gamma,
\end{equation}
where $N_{MC}^{\Znunu}$ is the expected number of \Znunu{} events obtained from simulation in a given search region, $R_Z$ is a factor used to account for any difference betweeen data and simulation in the cross section of the \Znunu{} process, and $S_\gamma$ accounts for any shape difference in the \Znunu{} process between data and simulation and is integrated in top and W tags. We use the same extrapolation method, see sec. \ref{sec:LL}, for the CR-to-SR transfer factor. The merged bins in $t$ and $W$ tags increases the limited statistics in these regions. 

The factor $R_Z$ is calculated by comparing the observed and expected \Zll{} yields after applying a relaxed definition of the baseline selection, with the $\Delta\phi(j,\metll)$ cut removed. The purity of the sample is improved by requiring the dilepton invariant mass to be near the \Z-mass window $(80<M_{ll}<100$ GeV) with a $\pt>200$ GeV cut for the entire system. Unfortunately the \ttbar{} contamination is not negligible in the $\nb\geq1$ region. To account for this we define a factor $R_T$ for \ttbar{} events that is calculated by using events outside the \Z-mass window $(50<M_{ll}<80$ or $M_{ll}>100$ GeV). The relation between the factors, $R_Z$ and $R_T$, and the observed and expected yields inside and outside the Z-mass window, is expressed as:
\begin{equation}
\begin{bmatrix}
\text{Data}_{\text{on}-Z} \\
\text{Data}_{\text{off}-Z}
\end{bmatrix}
=
\begin{bmatrix}
\text{MC}_{\text{on}-Z}(\Zll) & MC_{\text{on}-Z}(\ttbar) \\
\text{MC}_{\text{off}-Z}(\Zll) & MC_{\text{off}-Z}(\ttbar)
\end{bmatrix}
\cdot
\begin{bmatrix}
R_Z \\
R_T
\end{bmatrix}.
\end{equation}
The small contributions from $tZ, ttZ, WZ,$ and $ZZ$ and included in $R_Z$, while the processes $tW, ttW,$ and $WW$ are included in $R_T$. 

%To account for potential effects related to heavy flavor production, $R_Z$ and $R_T$ are calculated separately for different \nb{} and \nsv{} requirements. The high and low \dm{} baseline selections are applied for the corresponding regions, except for the requirements on the azimuthal angles between the leading jets and \met. Figures ? to ? show the data and simulated events in the regions used to calculate $R_Z$ and $R_T$, and results are summarized in Table ?. The statistical uncertainties on $R_Z$ are included in the systematic uncertainty on the final background prediction. 

%We utilize the $\gamma$+jets sample to extract corrections related to the modeling of the kinematics of \Znunu+jets events. The selected $\gamma$+jets sample consists of events that have at least one photon satisfying the selection described in Section 3.11. The quantity $S_\gamma$ is the shape correction factor that is calculated via a comparison of the $\met^\gamma$ distributions of $\gamma$+jets events in simulation and data. The simulation is normalized to the number of events seen in data after applying the baseline selections corresponding to the low and high \dm{} searches, respectively. A different $S_\gamma$ is estimated for each search region, to account for effects related to the search variables, and is used to correct the corresponding \Znunu+jets yields from simulation. To increase the statistical power of the correction, in the low \dm{} regions, we integrate over \nsv{} for $\nb=0$ and $\nb=1$ separately after verifying in simulation that there is no bias introduced in $\gamma$+jets $\met^\gamma$ distributions.

\subsection{Combination of Eras and Prediction}\label{subsec:znunucombine}

We want to make sure that the normalization and shape corrections in the electron and muon control regions are in relative aggreement to be able to combine them into a single normalization and shape correction. Figures \ref{fig:znunu-norm-lm-electron} and \ref{fig:znunu-norm-lm-muon} compare the normalized \metgamma{} distributions for $\gamma$+jets in data and simulation in each separate low \dm{} control region and era of the analysis. In Figures \ref{fig:znunu-norm-hm-electron} and \ref{fig:znunu-norm-hm-muon}, we have a comparison of the normalization of \metgamma{} distributions for $\gamma$+jets in data and simulation in each separate high \dm{} control region and era of the analysis. The agreement between each era and in each region is good which indicates that we can combine them before doing the normalization. The photon shape comparisons are shown in Figs. \ref{fig:znunu-shape-lm-photon} and \ref{fig:znunu-shape-hm-photon} for low and high \dm{}, respectively. The shape factors also show good agreement between each era to allow us to combine the eras and provide a single shape factor. 

A comparison between the electron and muon control regions and the combinations is shown in Fig. \ref{fig:znunu-norm-lm-comp} and \ref{fig:znunu-norm-hm-comp}. The shape correction is in good agreement among the eras so we are able to combine them all together. In Fig. \ref{fig:znunu-shape-lm-photon}, we have the shape correction in the low \dm{} photon control region separated into each era. Each of the eras are in good agreement so we should be able to combine all of the eras for each part of the analysis together. Tables \ref{tab:0l-zinv-pred-lm}, \ref{tab:0l-zinv-pred-hm-1}, \ref{tab:0l-zinv-pred-hm-2}, and \ref{tab:0l-zinv-pred-hm-3} summarize the yields of the \Znunu{} background and the prediction in each search region after applying the appropriate normalization and shape factors.  
\input{Znunu_Norm.tex}
\input{Znunu_Norm_Comp.tex}
\input{tables/yields_znunu_lm.tex}
\input{tables/yields_znunu_hm.tex}

%The events are selected online using the HLT\_Photon165\_HE10 trigger. The photon trigger efficiency is observed to degrade by up to ~ 10\% at high photon \pt{} due to an inefficiency in the L1 trigger. We therefore utilize the combination of triggers listed in Table 1 to recover efficiency. To suppress potential signal contamination and ensure that the control sample is independent from the search sample, we only consider events with $\met<200$ GeV. The $\gamma$+jets data control sample consists of three main components: prompt photons produced directly or via fragmentation, and fake photons. We combine the $\gamma$+jets and QCD simulated samples to estimate the contributions of these components to our control sample. We consider prompt photons to be reconstructed photons that are matched to a generator-level photon in space and momentum as follows: $\Delta R(\gamma_{gen}, \gamma_{reco})<0.1$ and $0.5<\pt^{gen}/\pt^reco<2$. We use the $\gamma$+jets simulation to describe the direct photon component and the QCD simulation to describe the fragmentation and fake components. We consider direct photons to be those that satisfy the relationship $\Delta R(\gamma, \text{parton})>0.4$, where parton refers to a quark of gluon at the generator level. To avoid double-counting events from the $\gamma$+jets and QCD simulated samples, we reject events in the QCD sample with $\Delta R(\gamma,\text{parton})>0.4$. Finally, reconstructed photons that are not matched to a generator-level photon are considered to be fakes. Finally, the $\gamma$+jets sample can also have a sizable contribution from $tt\gamma$ events in regions requiring at least one top- or W- have shown that significant variations in the normalization of each of these components in the $\gamma$+jets sample do not significantly affect the shapes of the search region variable distributions, and therefore it is sufficient to estimate the relative contributions of each component using the simulated samples.  

\section{Quantum Chromodynamic Events}
\label{subsec:QCD}

Simulation predicts negligible levels of QCD contamination in the various search regions. However, the QCD multijet simulation has limited statistics and there are uncertainties related to the description of physics in the simulation, particularly for the rare scenarios that would lead to a multijet event passing all of the final search region selection criteria. For these reasons, it is necessary to perform a data-driven QCD background estimation. We follow an approach similar to those described for other SM backgrounds, first using a QCD-enhanced control region and use it to validate the simulation, then extrapolate the event count in the control region to a prediction in the search region. 

\met{} is generated in QCD events through either jet \pt{} mis-measurement or semileptonic heavy flavor decay and for the purposes of this section both sources of \met{} will be generally referred to as "mis-measurement". This leads to the characteristic of \met{} being aligned to one of the leading jets, which motivates including a veto on such events in the baseline selection. On the other hand, inverting and tightening the \highdm{} selection from the high \dm{} region, or the \lowdm{} selection from the low \dm{} region, to \qcdcr{} for both the high and low \dm{} regions result in regions with fairly pure samples of QCD events. The QCD search region yields are estimated with data yields in a series of control regions with this modified baseline selection after subtracting the contamination of non-QCD processes. The control region yields are related to search region yields with the following simulation transfer factors:
\begin{equation}\label{QCDTF}
TF_{QCD}=\frac{N_{MC}^{QCD}(SR)}{N_{MC}^{QCD}(\qcdcr)},
\end{equation}
where $N_{MC}^{QCD}(SR)$ are the expected QCD yields from simulation for the signal regions, (\highdm{} for high \dm{} and \lowdm{} for low \dm) and $N_{MC}^{QCD}(\qcdcr)$ is the expected QCD yield from simulation for the control region. The QCD estimate, $N_{pred}^{QCD}$, is defined as:
\begin{equation}
N_{pred}^{QCD}=TF_{QCD}\cdot(N_{data}-N_{MC}^{non-QCD}),
\end{equation}
where $N_{data}$ is the number of events in the \qcdcr{} control sample described above, and $N_{MC}^{non-QCD}$ is the number of non-QCD events in this sample as estimated by the background predictions. This method is used to make the comparison with only the QCD events from data. The transfer factors in the high \dm{} region actually account for two levels of extrapolation, i.e., the extrapolation from the control regions to the search regions without the requirement of top and W tags, and the extrapolation in top and W tags in the search regions after correcting the top- and W-tagging efficiencies:
\begin{equation}\label{QCDTFExtrapolation}
\begin{split}
TF_{QCD}&=TF_{QCD}^{CR-SR}\times TF_{QCD}^{SR-extrap} \\
&=\frac{N_{MC}(SR)(\nj,\nb,\met)}{N_{MC}(\qcdcr)(\nj,\nb,\met)}\times\frac{N_{MC}(SR)(\nj,\nb,\met,\nt,\nrt,\nw)}{N_{MC}(SR)(\nj,\nb,\met)},
\end{split}
\end{equation}
where we have split the transfer function into two different parts, a control region to search region comparison and a search region to search region extrapolation.

The QCD control region estimation uses an extrapolation method for the high \dm{} region, similar to Sec. \ref{sec:LL}. The selection for these regions are the same except for the \nt, \nrt, and \nw{} tags, see Eqn. \ref{QCDTFExtrapolation}. This method allows for the measurement of the efficiency directly in data. This also allows for the statistical error on the estimation to be greatly improved. The lepton vetoes are still applied to the $N_{MC}^{non-QCD}$ and $N_{data}$, but not the $TF_{QCD}$ estimation. For the low \dm{} control region, it is binned in the same manner as the search region with the same cuts applied. Unfortunately, the QCD purity is low for the regions with one or more b-tags. To improve the statistics in these regions we merge the \met{} bins to improve the precision of the prediction in these regions. 

\subsection{QCD Local Smearing}\label{Smearing}
\input{QCD_jetResponse.tex}
The local smearing methods for QCD use a parameterization of the reconstructed (reco) jet. The reco jet is used when defining the jet response, see Eqn. \ref{jetres}. The analysis of the jet response spans many orders of magnitude and is binned in \pt{} and jet flavor, see Figs. \ref{fig:qcd-light-jet-res} and \ref{fig:qcd-b-jet-res}. 

The QCD events that fail the high \dm{} selection \highdm{} or the low \dm{} selection \lowdm, enter the search region due to a leading jet undergoing such severe mis-measurement that it is reconstructed as one of the sub-leading jets. Mis-measurement is parameterized by the jet response, defined as:
\begin{equation}\label{jetres}
r_{jet}=\frac{(\pt)_{reco}}{(\pt)_{gen}},
\end{equation}
where $(\pt)_{reco}$ is the \pt{} of the reco jet and $(\pt)_{gen}$ is the \pt{} of the gen-level jet that is matched to the reco jet.

Smearing is done by taking the response, Eqn. \ref{jetres}, and adjusting it using a small probability window. With the new response, we calculate what the new jet \pt{} for the new event using the gen-level jet \pt{} of the original event. This is done on the two reco jets that are matched to the leading and sub-leading gen-level jets. We then recalculate the missing energy parameters, but all other properties of the event are unaltered. We then store all events that pass, $\met>200$ GeV to reduce storage space. This allows for the improvement of the limited statistics of the original simulation. 

\subsection{QCD Corrections}

%\begin{figure}
%	\begin{center}
%  \includegraphics[width=0.8\textwidth]{QCD/qcd_res_tail_2016.png}
%  \includegraphics[width=0.8\textwidth]{QCD/qcd_res_tail_2017.png}
%  \includegraphics[width=0.8\textwidth]{QCD/qcd_res_tail_2018.png}
%	\end{center}
%	\caption[QCD Jet Response for each b-jet and light jet]{The correction for each event that has the corresponding b-jet or light jet that is then associated with the response of the jet. }
%	\label{fig:qcd-cr-response-corr}
%\end{figure}

%A much larger proportion of events in the search region than the control region are in the tail of the $r_{jet}$ distribution, which means that this method relies on the correct modeling of $r_{jet}$ in simulation. A $r_{jet}$ correction and uncertainty is extracted from data using the method described in Ref. \cite{cms_collaboration_search_2016}. In the case of \met{} being caused by the severe mis-measurement of a single jet, \met{} and that jet are aligned and the generated level transverse momentum of the jet is approximately the pseudo-generator level \pt. The correction is derived with the pseudo jet response $(r_{pseudo,jet})$ distribution, applying the QCD control region selection except for the number of jets and b-tagging requirements. We divide this region into two, in the case of the jet aligned to \met{} passing the medium b-tag requirement and in the case of it not passing the light b-tag requirement. The corrections range between $1.52\pm1.44$ to $1.53\pm2.44$, $0.13\pm1.08$ to $0.32\pm0.71$, and $0.82\pm0.29$ to $0.84\pm0.56$ in the case of jets originating from b-quarks and between $0.26\pm0.44$ to $1.66\pm0.46$, $0.87\pm0.24$ to $1.51\pm0.41$, and $0.98\pm0.27$ to $1.16\pm0.32$ for 2016, 2017, and 2018 respectively, and for all other jets as can be seen in Tables \ref{tab:qcd_tailsf-2016}, \ref{tab:qcd_tailsf-2017}, \ref{tab:qcd_tailsf-2018}.

%\input{tables/qcd_tail_sf.tex}

\begin{figure}
	\begin{center}
  \includegraphics[width=0.75\textwidth]{QCD/JetCorrection_2016.png} \\
  \includegraphics[width=0.75\textwidth]{QCD/JetCorrection_2017.png} \\
	\end{center}
	\caption[QCD Jet Response Scale Factor]{The scale factor for the smeared QCD when comparing the data to simulation for 2016 and 2017.}
	\label{fig:qcd-cr-response-sf-1}
\end{figure}

\begin{figure}
	\begin{center}
  \includegraphics[width=0.75\textwidth]{QCD/JetCorrection_2018PreHEM.png} \\
  \includegraphics[width=0.75\textwidth]{QCD/JetCorrection_2018PostHEM.png} \\
	\end{center}
	\caption[QCD Jet Response Scale Factor]{The scale factor for the smeared QCD when comparing the data to simulation for 2018 pre-HEM and 2018 post-HEM.}
	\label{fig:qcd-cr-response-sf-2}
\end{figure}

\begin{figure}
	\begin{center}
  \includegraphics[width=0.75\textwidth]{QCD/QCD_corr_2016.png} \\
  \includegraphics[width=0.75\textwidth]{QCD/QCD_corr_2017.png} \\
	\end{center}
	\caption[QCD Jet Response Correction]{The correction for each event that has the corresponding b-jet or light jet that is then associated with the response of the jet. }
	\label{fig:qcd-cr-response-corr-1}
\end{figure}

\begin{figure}
	\begin{center}
  \includegraphics[width=0.75\textwidth]{QCD/QCD_corr_2018preHEM.png} \\
  \includegraphics[width=0.75\textwidth]{QCD/QCD_corr_2018postHEM.png} \\
	\end{center}
	\caption[QCD Jet Response Correction]{The correction for each event that has the corresponding b-jet or light jet that is then associated with the response of the jet. }
	\label{fig:qcd-cr-response-corr-2}
\end{figure}

The QCD jet response, $r_{jet}$, is known to have a mismatch between data and simulation. To account for this, we devide the $r_{jet}$ distribution into five bins which are then normalized to the data, see Fig. \ref{fig:qcd-cr-response-sf-1} and \ref{qcd-cr-response-sf-2}. The scale factor for 2018 post-HEM is large but the $r_{jet}$ is in good agreement after the correction. The $r_{jet}$ distribution after corrections is show in Fig. \ref{fig:qcd-cr-response-corr-1} and \ref{fig:qcd-cr-response-corr-2}. We can see each era has a good agreement between data and simulation.

\subsection{QCD Prediction}\label{sec:QCDPred}

The statistical uncertainty on $TF_{QCD}$ is reduced by increasing the effective luminosity of the QCD multijet sample with a method referred to as "local smearing." The method relies on the parameterization of $r_{jet}$, which is only dependent on jet properties. Tables \ref{tab:0l-qcd-pred-lm}, \ref{tab:0l-qcd-pred-hm-1}, \ref{tab:0l-qcd-pred-hm-2}, and \ref{tab:0l-qcd-pred-hm-3} summarize the yields in data, the derived transfer factors, and the resulting QCD predictions in all \datalumi{} of Run 2 data for the high \dm{} and low \dm{} search regions, respectively. 

\input{QCD_LM_CR_plots.tex}
\input{QCD_HM_CR_plots.tex}
\input{tables/yields_qcd_lm.tex}
\input{tables/yields_qcd_hm.tex}

\section{Rare Interactions}
\label{subsec:rare}

\begin{figure}
	\begin{center}
  \includegraphics[width=0.4\textwidth]{rare/TTZ.png}
	\end{center}
	\caption[Diboson and $ttZ$ Feynman Diagrams]{The first order Feynman diagrams that are responsible for the main rare background.}
	\label{fig:rare-feynman-diagrams}
\end{figure}

The contributions of diboson, WW, WZ, and ZZ, processes are relatively small compared to the other backgrounds, but they mainly affect the search regions targeting low \dm{} signal models. The contribution of the $ttZ$ background is also generally very small due to the two vertex factor suppression of the leading-order process, see Fig. \ref{fig:rare-feynman-diagrams}. Although, in search regions that require the presence of more than one top- or W-tagged candidate, this process can be a significant fraction of the total SM background due to the suppression of the other SM processes. We define a three-lepton control sample that requires the presence of exactly three electrons or muons satisfying \pt$>40$ GeV for the leading lepton, \pt$>20$ GeV for the second and third leptons, and no additional lepton with \pt$>10$ GeV with an additional requirement of at least five jets, at least two of which are b-tagged. 

A Z boson mass window of 81 to 101 GeV is placed on the invariant mass of the same-flavor dilepton \pt{} of the lepton pair is further required to be at least 100 GeV, in order to probe a kinematic region close to the one relevant for the analysis. In order to check the extrapolation from the lower \Z-\pt{} region of this control sample to the search sample, we evaluate the $ttZ$ scale factor in bins of reconstructed \Z{} boson \pt{} as far as statistics permit. The \pt-binned scale factors are found to be consistent with the inclusive scale factor evaluated for $\pt(Z)>100$ GeV. Figure \ref{fig:rare-norm-comp} shows the comparison between 2016, 2017, and 2018 in the muon control region. Since there is relatively good agreement between the eras we can indeed combine them into a single estimation. The final estimation is included in the datacards in Sec. \ref{sec:Combination}

\input{ttZ_CR_comp.tex}

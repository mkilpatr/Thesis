\chapter{Top Squark Production and Backgrounds}
\label{ch:Search}

We have now motivated that the top squark, \st{}, could be the lightest squark, see Sec. \ref{subsec:StopMassSpec}. This allows us to possibly produce them at CMS. In Sec. \ref{sec:Baseline}, we have designed a search that will look for events with large amounts of \met{} and \nj. These events are being targeted in separate low \dm{} and high \dm{} search regions. For all of these we are interested in comparing the data and background in each search or control region. We will look at the production and decay modes of various \st{} interactions and the estimation of the SM background of each region. 

\section{Production and Decay Modes}
\label{sec:Production}

To produce the top squark all we need is the collision of two proton-proton beams. It is shown as a black circle in the Fig. \ref{fig:stop-gluino-production} and \ref{fig:stop-direct-production}. This is meant to represent many processes that can make a top squark. The main processes are gluon fusion, when two gluons fuse into a single gluon which then decays into a top and anti-top squark pair, or annihilation, where two quarks annihilate to a gluon propagator which thus decays into two top squarks. 

\input{StopSUSYModes.tex}

The main decay mode of the top squark is $\st\rightarrow t+\neutralino$ and $\st\rightarrow b+\chargino$, Fig. \ref{fig:stop-direct-production}. The top quark most likely decays as, $t\rightarrow b\W^+$, while the $b$ quark will decay to either a $c$ or an $u$ quark in its decay chain with an additional \W{} boson. The \neutralino{} is proposed to be a stable dark matter candidate while the \chargino{} could decay as, $\chargino\rightarrow\neutralino\W$. Next, a four body decay is allowed for, $\st\rightarrow b f f^\prime \neutralino$, see Fig. \ref{fig:stop-direct-production}(d,e). The final direct \st{} production we are interested in is the $\st\rightarrow c\neutralino$, see Fig. \ref{fig:stop-direct-production}(f). To be as inclusive as possible, we are also including indirect top squark production as see in Fig. \ref{fig:stop-gluino-production}. GET A CONCISE DESCRIPTION OF THE INDIRECT DIAGRAMS. We see that the \st{} will decay to multiple jets, \nj, and missing transverse energy, \met. Now we are going to try to estimate the SM background that could be in each of our search region bins.

\section{Standard Model Background}
\label{sec:SMBackground}

The standard model background for the top squark search is defined by a large \met{} and a multiple jets. The are a couple types of SM background that can be misinterpretted as our signal. The most likely background is that which causes many tops (or heavy particles) and missing energy. Events in the SM like \ttbar{} and \W+jets will have many jets projuced and \met{} due to a missed lepton and neutrino. The production of heavy particles like \Znunu{} will give mutliple jets and \met{} from the neutrinos being missed by the detector. QCD events often produce events with multiple jets, due to acceptances in the detector the jets can sometimes be mis-measured which can cause large \met{}. There are also various processes that are quite rare which are quite rare but still need to be accounted for. 

\section{Lost Lepton}
\label{sec:LL}

The contribution from the \ttbar{} and \W+jets processes arises from leptonic decays of the \W{} boson, where the charged lepton is outside the kinematic acceptance of CMS or evades identificaiton by the dedicated lepton vetoes. Large \met{} can be generated by the associated neutrino and the lepton that is not reconstructed, allowing such events to enter the search regions. This background is collectively referred to as the "Lost Lepton" (LL) background. Contributions arising from $tW, ttW$ and single-top processes also enter into this category, but with much smaller importance. 

Studies in simulation indicate that the event kinematics for different lepton flavors are similar enough to allow us to estimate them collectively from a single control sample in data that has event characteristics similar to those of the search sample. Because of this, we use the single-lepton control sample to estimate the LL background, using the method described in detail in Ref. \cite{bravo_search_2015}. The single-lepton sample consists of events that have one lepton satisfying the lepton-veto criteria. In order to suppress potential signal contamination, we require $M_T(l,\met)<100~\GeV$. The requirement of low $M_T(l,\met)$ also ensures orthogonality to the search regions used in direct top squark production in the single-lepton or double-lepton final state, making it possible to statistically combine the results of the two searches. The selection applied to the single-lepton control sample follows the same selection on the search variables as in the zero-lepton selection with the exception of classification according to the number of top and \W-tagged candidates. 

\subsection{Combining All Run 2 Eras}\label{sec:LLCombination}
Firstly, for this analysis we are interested in the possibility in combining the yields of each era into one estimation. This is initially done by looking at the \met{} distributions in each era. Since the LL estimation is done with the transfer factor method, a good confirmation would be the comparison of the tranfer factor in each SR for each era. 
\input{LostLepton_Inclusive_Combination.tex}

\subsection{Transfer Factors}
\label{subsec:TF}

The LL estimation in each search region is based upon the event count in data in the corresponding control region in the single-lepton sample. The count is extrapolated to the search region to obtain a prediction by means of a transfer factor obtained from simulation as follows: 

\begin{equation}
\label{eqn:LLTF}
N_{pred}^{LL}=TF_{LL} \cdot N_{data}(1l).
\end{equation}

This allows us to have the same selection for the single-lepton control sample and the zero-lepton sample. The only exception is the number of top and W-tagged candidates. The LL estimation is dependent on the yield of data in the corresponding CR and the TF calculated by the single-lepton sample. The transfer factor is defined as, 
\begin{equation}
\label{eqn:TF}
TF_{LL}=\frac{N_{MC}(0l)}{N_{MC}(1l)},
\end{equation}
where $N_{MC}(1l)$ is the event count observed in the corresponding CR and $N_{MC}(0l)$ use the event count in the corresponding SR. 

The main motivation behind this approach is to increase the statistical precision of the background estimation. The performance of the $t$ and \W{} taggers has been studied in data and MC samples and a reasonably good agreement is observed allowing us to proceed with this approach. Data-to-MC scale factors are extracted and applied to MC to account for residual differences of the tagging performance in data. Detailed studies comparing the performance of the $t$ and \W{} taggers in data and MC \cite{noauthor_search_nodate}.

The control regions utilized to predict the LL background are displayed in Fig. \ref{fig:llb-1lcr-datavsmc-lm-nb0} to \ref{fig:llb-1lcr-datavsmc-hm-nb3-1}. The figures \ref{fig:llb-1lcr-datavsmc-lm-nb0} to \ref{fig:llb-1lcr-datavsmc-lm-nb1} display the control regions specific to the low \dm{} selection, where the regions are binned following the search region definition. Figures \ref{fig:llb-1lcr-datavsmc-hm-nt0-nrt0-nw0} to \ref{fig:llb-1lcr-datavsmc-hm-nb3-1} display the control regions dedicated for the high \dm{} selection. Due to the nature of the background estimation method applied in the high \dm{} search, control regions are utilized for the prediction of multiple search regions. 

Tables \ref{tab:0l-llb-pred-lm} to \ref{tab:0l-llb-pred-hm-3} summarize the yields in data observed in the single-lepton sample, the derived transfer factor, and the resulting LL predictions for the low \dm{} and high \dm{} search regions respectively. The transfer factors in the high \dm{} region actually account for two levels of extrapolation. The CR for the high \dm{} is loose such that, there is no binning in tops or W tagging. We then extrapolate to the SR with the inclusion of the top and W tags, along with scale factors, to estimate the LL background in the region:
\begin{equation}\label{TFExtrapolation}
\begin{split}
TF_{LL}=& TF_{LL}^{CR-SR}\times TF_{LL}^{SR-extrap} \\
=& \frac{N_{MC}(0l)(\nj,\nb,\met)}{N_{MC}(1l)(\nj,\nb,\met)}\times\frac{N_{MC}(0l)(\nj,\nb,\met,\nt,\nrt,\nw)}{N_{MC}(0l)(\nj,\nb,\met)}.
\end{split}
\end{equation}

We now want to consider how the transfer factor for each era relates to the total transfer factor. In Fig. \ref{fig:llb-1lcr-datavsmc-total-tf} and \ref{fig:llb-1lcr-datavsmc-sep-tf}, we see the comparison the the total $TF$ for each era of the data and simulation. these are all in quite good agreement, but we see are large peak near zero. Once we alter the comparison for the $TF$ for the CR-to-SR and the SR-to-Extrapolation. We see a much better agreement when we do not include the cuts on top/W tagging. These are improved because of the better statistics in the region. 
\input{LostLepton_TransferFactor_Comparison.tex}
\input{tables/yields_llb_all_lm.tex}
\input{tables/yields_llb_all_hm.tex}
\input{LostLepton_LM_CR_plots.tex}
\input{LostLepton_HM_CR_plots.tex}

Now that we have confirmed that the LL background is valid to be combined to be combined for all the eras. The transfer factors and control region comparisons confirm that each era can be combined in the final estimation. Now with the combinations of each era for the LL back ground, we can consider the combination of the other background estimations. 

\section{Z Boson Decay to Neutrinos}
\label{subsec:Znunu}

An important source of background for the zero-lepton search is from events in which a \Z{} boson, produced in association with jets, decays to neutrinos that result in a significant amount of missing energy in the event. Two methods are traditionally used to estimate the \Znunu{} background. The first method makes use of a sample dominated by \Zll+jets events. This approach comes with the advantage of very similar kinematics (after correcting for the difference in acceptance between charged lepton pairs and pairs of neutrinos), but is statistically limited, especially in the tight search regions used in SUSY searches. The second method utilizes a $\gamma+$jets sample. The $\gamma+$jets process has a factor of 5 or more larger cross section than the \Zll+jets process, and has similar leading order Feynman diagrams to \Z+jets events. However, there are two main differences between the two processes that must be taken into account, namely, different quark-boson couplings and the fact that the \Z{} boson is very massive. Both of these effects become less important with higher boson \pt, which is the kinematic region we are probing with this search. The \met{} of the $\gamma+$jets process is calculated after removing the photon from the event to mimic the \Znunu{} process.

Based on the above, we use a hybrid method to estimate the \Znunu{} background that makes use of both the $\gamma+$jets and the \Zll+jets processes. The photon and the dilepton system are removed from the events before calculating \met{} and other kinematic variables related to \met, and the modified \met{} is denoted by $\met^\gamma$ and $\met^{ll}$ for $\gamma+$jets and the \Zll+jets processes, respectively. We utilize the \Zll+jets sample to measure the normalization of the \Znunu{} process in different ranges of \nb{} and \nsv, and we take advantage of the much higher statistics of the $\gamma+$jets sample to extract shape corrections. As discussed in Sec. \ref{sec:LL}, the good agreement we observe between data and simulation in the Lost Lepton background leads us to integrate the control regions used in the estimation of the \Znunu{} background in the number of $t$ and \W{} tags to increase the statistical power of the prediction. We then extrapolate into tagged regions using simulation, corrected with the appropriate $t$ and \W{} tagging data-to-simulation scale factors.

The prediction of the \Znunu{} background is given by:
\begin{equation}
N_{pred}^{\Znunu}=N_{MC}^{\Znunu} \cdot R_Z \cdot S_\gamma
\end{equation}

Znunu: production of a Z boson that decays into two nuetrinos which are then missed by the detector. Can have jets from other quarks/gluons in the interaction

\section{Quantum Chromodynamic Events}
\label{subsec:QCD}

Simulation predicts negligible levels of QCD contamination in the various search regions. However, the QCD multijet simulation has limited statistics and there are uncertainties related to the description of physics in the simulation, particularly for the rare scenarios that would lead to a multijet event passing all of the final search region selection criteria. For these reasons, it is necessary to perform a data-driven QCD background estimation. We follow an approach similar to those described for other SM backgrounds, first using a QCD-enhanced control region and use it to validate the simulation, then extrapolate the event count in the control region to a prediction in the search region. 

\met{} is generated in QCD events through either jet \pt{} mis-measurement or semileptonic heavy flavor decay and for the purposes of this section both sources of \met{} will be generally referred to as "mis-measurement". This leads to the characteristic of \met{} being aligned to one of the leading jets, which motivates including a veto on such events in the baseline selection. On the other hand, inverting and tightening the \highdm{} selection from the high \dm{} region, or the \lowdm{} selection from the low \dm{} region, to \qcdcr{} for both the high and low \dm{} regions result in regions with fairly pure samples of QCD events. The QCD search regions yields are estimated with data yields in a series of control regions with this modified baseline selection after subtracting the contamination of non-QCD processes. The control region yields are related to search region yields with the following simulation transfer factors:
\begin{equation}\label{QCDTF}
TF_{QCD}=\frac{N_{MC}^{QCD}(SR)}{N_{MC}^{QCD}(\qcdcr)}
\end{equation}
where $N_{MC}^{QCD}(SR)$ are the expected QCD yields from simulation for the signal regions, (\highdm{} for high \dm{} and \lowdm{} for low \dm) and $N_{MC}^{QCD}(\qcdcr)$ is the expected QCD yield from simulation for the control region. The QCD estimate, $N_{pred}^{QCD}$, is defined as:
\begin{equation}
N_{pred}^{QCD}=TF_{QCD}\cdot(N_{data}-N_{MC}^{non-QCD}),
\end{equation}
where $N_{data}$ is the number of events in the \qcdcr{} control sample described above, and $N_{MC}^{non-QCD}$ is the number of non-QCD events in this sample as estimated by the background predictions. This method is used to make the comparison with only the QCD events from data. The transfer factors in the high \dm{} region actually account for two levels of extrapolation, i.e., the extrapolation from the control regions to the search regions without the requirement of top and W tags, and the extrapolation in top and W tags in the search regions after correcting the top- and W-tagging efficiencies:
\begin{equation}\label{QCDTFExtrapolation}
\begin{split}
TF_{QCD}&=TF_{QCD}^{CR-SR}\times TF_{QCD}^{SR-extrap} \\
&=\frac{N_{MC}(SR)(\nj,\nb,\met)}{N_{MC}(\qcdcr)(\nj,\nb,\met)}\times\frac{N_{MC}(SR)(\nj,\nb,\met,\nt,\nrt,\nw)}{N_{MC}(SR)(\nj,\nb,\met)},
\end{split}
\end{equation}
where we have split the transfer function into two different parts, a control region to search region comparison and a search region to search region extrapolation.

The QCD control region estimation uses an extrapolation methods for the high \dm{} region. The selection for these regions are the same except for the \nt, \nrt, and \nw{} tags, see Eqn. \ref{QCDTFExtrapolation}. This method allows for the measurement of the efficiency directly in data. This also also for the statistical error on the estimation fo be greatly improved. The lepton vetoes are still applied to the $N_{MC}^{non-QCD}$ and $N_{data}$, but not the $TF_{QCD}$ estimation. For the low \dm{} control region, it is binned in the same manner as the search region with the same cuts applied. Unfortunately, the QCD purity is low for the regions with one or more b-tags. To improve the statistics in these regions we merge the \met{} bins to improve the precision of the prediction in these regions. 

\subsection{QCD Local Smearing}\label{Smearing}
\input{QCD_jetResponse.tex}
The local smearing methods for QCD use a parameterization of the reconstructed(reco) jet. The reco jet is used when defining the jet response, see Eqn. \ref{jetres}. The analysis of the jet response spans many order of mangitude and is binned in \pt{} and jet flavor, see Fig. \ref{fig:qcd-light-jet-res} and \ref{fig:qcd-b-jet-res}. 

The QCD events that fail the \highdm{} or \lowdm{} selection cuts, for the high and low \dm{} regions respectively, enter the search region due to a leading jet undergoing such severe mis-measurement that it is reconstructed as one of the sub-leading jets. Mis-measurement is parameterized by the jet response, defined as:
\begin{equation}\label{jetres}
r_{jet}=\frac{(\pt)_{reco}}{(\pt)_{gen}},
\end{equation}
where $(\pt)_{reco}$ is the \pt{} of the reconstructed jet and $(\pt)_{gen}$ is the \pt{} of the gen-level jet that is matched to the reconstructed jet.

\subsection{QCD Corrections}

Figures ? and ? show the \met{} distribution in data and simculation in the QCD control regions for the high \dm{} and lowd \dm{} selections, respectively. The stacked plot labeled "Non-QCD bkg" is the distribution of the non-QCD events as predicted with the estimation methods detailed in this note. The other stacked plot, labeled "Smeared QCD MC" is the smeared QCD simulation yield after applying the $r_{jet}$ corrections. The other two lines show a combination of the predicted non-QCD SM yields and one of two scenarios. "Without $r_{jet}$ corr" is the smeared QCD simulation without the $r_{jet}$ correction and "With orig. QCD MC" is the standard QCD simulation without this correction. There are three important trends in this figure. The first is the purity of the control regions, which gives confidence in using them to predict QCD yields. The second is that the "Without $r_{jet}$ corr" estimation is nearly equivalent to "With orig. QCD MC" in the regions where there are enough statistics to have a meaningful comparison. This improves our confidence in the use of the smeared QCD simulation. Finally, the $r_{jet}$ correction improves the agreement between data simulation, which is a useful validation of this correction. Due to the nature of the background estimation method applied in the high \dm{} search, control regions are utilized for the prediciton of multiple search regions. 

\begin{figure}
	\begin{center}
  \includegraphics[width=0.8\textwidth]{QCD/qcd_res_tail_2016.png}
  \includegraphics[width=0.8\textwidth]{QCD/qcd_res_tail_2017.png}
  \includegraphics[width=0.8\textwidth]{QCD/qcd_res_tail_2018.png}
	\end{center}
	\caption[QCD Jet Response for eah b-jet and light jet]{The correction for each event that has the corresponding b-jet or light jet that is then associated with the response of the jet. }
	\label{fig:qcd-cr-response-corr}
\end{figure}

A much larger proportion of events in the search region than the control region are in the tail of the $r_{jet}$ distribution, which means that this method relies on the correct modeling of $r_{jet}$ in simulation. Therefore, a $r_{jet}$ correction and uncertainty is extracted from data. In the case of \met{} being caused by the severe mis-measurement of a single jet, \met{} and that jet are aligned and the generated level transverse momentum of the jet is approximately the pseudo-generator level \pt.

A much larger proportion of events in the search region that the control region are in the tail of the $r_{jet}$ distribution, which means that this method relies on the correct modeling of $r_{jet}$ in simulation. A $r_{jet}$ correction and uncertainty is extracted from data using the method described in Ref. []. The correction is derived with the pseudo jet response $(r_{pseudo,jet})$ distribution, applying the QCD control region selection except for the number of jets and b-tagging requirements. We divide this region into two, in the case of the jet aligned to \met{} passing the medium b-tag requirement and in the case of it not passing the light b-tag requirement. The corrections range between $1.52\pm1.44$ to $1.53\pm2.44$, $0.13\pm1.08$ to $0.32\pm0.71$, and $0.82\pm0.29$ to $0.84\pm0.56$ in the case of jets originating from b-quarks and between $0.26\pm0.44$ to $1.66\pm0.46$, $0.87\pm0.24$ to $1.51\pm0.41$, and $0.98\pm0.27$ to $1.16\pm0.32$ for 2016, 2017, and 2018 respectively, and for all other jets as can be seen in Tables \ref{tab:qcd_tailsf-2016}, \ref{tab:qcd_tailsf-2017}, \ref{tab:qcd_tailsf-2018}.

\input{tables/qcd_tail_sf}

The statistical uncertainty on $TF_{QCD}$ is reduced by increasing the effective luminosity of the QCD multijet sample with a method referring to as "local smearing." The method relies on the parametrization of $r_{jet}$, which is only dependent on jet properties.

Tables ? and ? summarize the yields in data, the derived transfer factors, and the resulting QCD predictions for the high \dm{} and low \dm{} search regions respectively. 

\input{QCD_LM_CR_plots.tex}
\input{QCD_HM_CR_plots.tex}

\section{Rare Interactions}
\label{subsec:rare}

The contributions of diboson (WW, WZ, and ZZ) processes are relatively small compared to the other backgrounds, and mainly affect the search regions targeting low \dm{} signal models. The prediciton of the diboson background is obtained directly from simulation, and an uncertainty of 50\% is assigned on the cross section.

The contribution of the ttZ background is also generally very small due to the rarity of this process. However, in search regions that require the presence of more than one top- or W-tagged candidates, this process can constitute a significant fraction of the total SM background due to the strong suppression of the other SM processes. In order to validate the prediction of this background, we define a three-lepton control sample, selected using single-lepton triggers, which requires the presence of exactly three electrons or muons satisfying \pt$>40$ GeV for the leading lepton, \pt$>20$ GeV for the second and third leptons, and no additional lepton with \pt$>10$ GeV. We further require at least five jets, at least two of which are b-tagged. A Z boson mass window of 81-101 GeV is placed on the invariant mass of the same-flavor dilepton \pt{} of this lepton pair is further required to be at least 100 GeV, in order to probe a kinematic region close to the one relevant for the analysis. Figure ? shows the reconstructed Z boson \pt{} distribution observed in this sample. We use the region outside the Z boson mass window (the \met{} distribution in this region is also shown in Fig. ?) to simultaneously constrain the \ttbar background consisting of dilepton \ttbar events with an additional lepton originating from semi-leptonic b-hadron decay or from a misidentified jet, and obtain a scale factor of $1.10\pm0.26$ for the ttZ-like processes in this sample, where the uncertainty is dominated by the statistical uncertainty in the data sample. We therefore apply an uncertainty of 24\% to the normalization of the ttZ background in the analysis. In order to check the extrapolation from the lower Z-\pt{} region of this control sample to the search sample, we evaluate the ttZ scale factor in bins of reconstructed Z boson \pt{} as far as statistics permit. The \pt-binned scale factors are found to be consistent with the inclusive scale factor evaluated for $\pt(Z)>100$ GeV (Fig. sldkjf). Additional experimental and theoretical uncertainties related to PDF and factorization/renormalization scale variations are also assigned. Figure ? shows the number of top- and W-tagged events observed in the ttZ control sample. 

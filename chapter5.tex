\chapter{Stop quark Production and Backgrounds}
\label{ch:Search}

\input{StopSUSYModes.tex}

\section{Production and Decay Modes}
\label{sec:Production}

Gluon fusion.

Main decay mode mode $\st\rightarrow t+\neutralino$, $\st\rightarrow b+\chargino$. The top quark most likely decays into a b quark and W boson. 

\section{Standard Model Background}
\label{sec:SMBackground}

Signal events can be mimicked by SM events that have a large number of jets and missing energy. 

Broken up into four major backgrounds, Lost Lepton (LL), Znunu, QCD, Rare decays

\subsection{Lost Lepton}
\label{subsec:LL}

The contribution from the \ttbar{} and \W+jets processes arises from leptonic decays of the \W boson where the charged lepton is outside the kinematic acceptance of CMS or evades identificaiton by the dedicated lepton vetoes. Large \met can be generated by the associated neutrino and the lepton that is not reconstructed, allowing such events to enter the search regions. This background is collectively referred to as the "Lost Lepton" (LL) background. Contributions arising from $ttW$ and single-top processes also enter into this category, but with much smaller importance. 

Studies in simulation indicate that the event kinematics for different lepton flavors are similar enough to allow us to estimate them collectively from a single control sample in data that has event characteristics similar to those of the search sample. Because of this, we use the single-lepton control sample to estimate the LL background, using the method descrived in detail in Ref. [18]. The single-lepton sample consists of events that have one lepton satisfying the lepton-veto criteria. In order to suppress potential signal contamination, we require $M_T(l,\met)<100 \GeV$. If there is more than one selected lepton, we randomly select which lepton is chosen to determine the $M_T(l,\met)$. The requirement of low $M_T(l,\met)$ also ensures orthogonality to the search regions used in the search for direct top squark production in the single-lepton final state, making it possible to statistically combine the results of the two searches. The selection applied to the single-lepton control sample follows the same selection on the search variables as in the zero-lepton selection with the exception of classification according to the number of top and \W-tagged candidates. 

\subsection{Combining All Run 2 Eras}\label{sec:LLCombination}
Firstly, for this analysis we are interested in the possibility in combining the yields of each era into one estimation. This is initially done by looking at the \met{} distributions in each era, see Fig. \ref{fig:llb-1lcr-datavsmc-lm-inclusive} and \ref{fig:llb-1lcr-datavsmc-hm-inclusive}. These all have a good agreement between Data and MC. 
\input{LostLepton_Inclusive_Combination.tex}

\subsubsection{Transfer Factors}
\label{subsec:TF}

The LL estimation in each search region is based upon the event count in data in the corresponding control region in the single-lepton sample. The count is extrapolated to the search region to obtain a prediction by means of a transfer factor obtained from simulation samples as follows: 

\begin{equation}
\label{eqn:LLTF}
N_{pred}^{LL}=TF_{LL} \cdot N_{data}(1l).
\end{equation}

We want to suppress signal contamination by requiring $M_{T}(l, \met) < 100$ GeV. This requirement confirms that it is orthogonal to the search reagions that are used in the search for direct top squark production in the single-lepton final state. Letting the two anaysis statistically combine the results in the future. 

This allows us to have the same selection for the single-lepton control sample and the zero-lepton sample. The only exception is the number of top and W-tagged candidates? what is the difference betweeen a candidate and a particle?

The LL estimation is dependent on the yield of data in the corresponding CR and the TF calculated by the single-lepton sample. The transfer factor is defined as, 
\begin{equation}
\label{eqn:TF}
TF_{LL}=\frac{N_{MC}(0l)}{N_{MC}(1l)},
\end{equation}
where $N_{MC}(1l)$ is the event count observed in the corresponding CR and $N_{MC}(0l)$ use the event count in the corresponding SR. 

We now want to consider how the transfer factor for each era relates to the total transfer factor. 
\input{LostLepton_TransferFactor_Comparison.tex}

\input{../Research/SUSY/2019/LLB/lepcr_allEras/yields/yields_llb_all_lm.tex}
\input{../Research/SUSY/2019/LLB/lepcr_allEras/yields/yields_llb_all_hm.tex}
\input{LostLepton_LM_CR_plots.tex}
\input{LostLepton_HM_CR_plots.tex}

\subsection{Z Boson Decay to Neutrinos}
\label{subsec:Znunu}

An important source of background for the zero-lepton search is from events in which a \Z{} boson, produced in association with jets, decays to neutrinos that result in a significant amount of missing energy in the event. Two methods are traditionally used to estimate the \Znunu{} background. The first method makes use of a sample dominated by \Zll+jets events. This approach comes with the advantage of very similar kinematics (after correcting for the difference in acceptance between charged lepton pairs and pairs of neutrinos), but is statistically limited, especially in the tight search regions used in SUSY searches. The second method utilizes a $\gamma+$jets sample. The $\gamma+$jets process has a factor of 5 or more larger cross section than the \Zll+jets process, and has similar leading order Feynman diagrams to \Z+jets events. However, there are two main differences between the two processes that must be taken into account, namely, different quark-boson couplings and the fact that the \Z{} boson is very massive. Both of these effects become less important with higher boson \pt, which is the kinematic region we are probing with this search. The \met{} of the $\gamma+$jets process is calculated after removing the photon from the event to mimic the \Znunu{} process.

Based on the above, we use a hybrid method to estimate the \Znunu{} background that makes use of both the $\gamma+$jets and the \Zll+jets processes. The photon and the dilepton system are removed from the events before calculating \met{} and other kinematic variables related to \met, and the modified \met{} is denoted by $\met^\gamma$ and $\met^{ll}$ for $\gamma+$jets and the \Zll+jets processes, respectively. We utilize the \Zll+jets sample to measure the normalization of the \Znunu{} process in different ranges of \nb{} and \nsv, andwe take advantage of the much higher statistics of the $\gamma+$jets sample to extract shape corrections. As discussed in Sec. , the good agreement we observe between data and simulation in the Lost Lepton background leads us to integrate the control regions used in the estimation of the \Znunu{} background in the number of $t$ and \W{} tags to increase the statistical power of the prediction. We then extrapolate into tagged regions using simulation, corrected with the appropriate $t$ and \W{} tagging data-to-simulation scale factors.

The prediction of the \Znunu{} background is given by:
\begin{equation}
N_{pred}^{\Znunu}=N_{MC}^{\Znunu} \cdot R_Z \cdot S_\gamma
\end{equation}

Znunu: production of a Z boson that decays into two nuetrinos which are then missed by the detector. Can have jets from other quarks/gluons in the interaction

\subsection{Quantum Chromodynamic Events}
\label{subsec:QCD}

Simulation predicts negligible levels of QCD contamination in the various search regions. However, the QCD multijet simulation has limited statistics and there are uncertainties related to the description of physics in the simulation, particularly for the rare scenarios that would lead to a multijet event passing all of the final search region selection criteria. For these reasons, it is necessary to perform a data-driven QCD background estimation. We follow an approach similar to those described for other SM backgrounds, first using a QCD-enhanced region to validate the simulation, then extrapolating the event count in the control region to a prediction in the search region. 

\met{} is generated in QCD events through either jet \pt{} mis-measurement or semileptonic heavy flavor decay and for the purposes of this section both sources of \met{} will be generally referred to as "mis-measurement". This leads to the characteristic of \met{} being aligned to one of the leading jets, which motivates including a veto on such events in the baseline selection. On the other hand, inverting and tightening the \qcdhighdm{} selection from the high \dm{} region, or the \qcdlowdm{} selection from the low \dm{} region, to \qcdcr{} for both the high and low \dm{} regions result in regions with fairly pure samples of QCD events. The QCD search regions yields are estimated with data yields in a series of control regions with this modified baseline selection after subtracting the contamination of non-QCD processes. The control region yields are related to search region yields with the following simulation transfer factors:
\begin{equation}\label{QCDTF}
TF_{QCD}=\frac{N_{MC}^{QCD}(SR)}{N_{MC}^{QCD}(\qcdcr)}
\end{equation}
where $N_{MC}^{QCD}(SR)$ are the expected QCD yields from simulation for the signal regions, (\qcdhighdm{} for high \dm{} and \qcdlowdm{} for low \dm) and $N_{MC}^{QCD}(\qcdcr)$ is the expected QCD yield from simulation for the control region. The QCD estimate, $N_{pred}^{QCD}$, is defined as:
\begin{equation}
N_{pred}^{QCD}=TF_{QCD}\cdot(N_{data}-N_{MC}^{non-QCD}),
\end{equation}
where $N_{data}$ is the number of events in the \qcdcr{} control sample described above, and $N_{MC}^{non-QCD}$ is the number of non-QCD events in this sample as estimated by the background predictions.

For the estimation of the QCd contribution in the high \dm{} search regions, the QCD control regions match the selection in the corresponding search regions except from the selection on $N_t,N_W,$ and $N_{res}$. Binning the control regions in these dimensions has the advantage of measureming the efficiency of each variable directly in data. The $t$ and \W{} tags improves significantly the statistical power of the estimate. The lepton vetoes are not applied when calculating $TF_{QCD}$, only on $N_{data}$ and $N_{MC}^{non-QCD}$. The fake rates are then estimated directly from data. 

A similar approach is utilized for the estimation of the QCD contribution in the low \dm{} search regions. The QCD control regions are binned in the same variables and ranges as of the search regions, but the QCD purity is low for the low \dm{} control regions with one or more b-tags (the high \dm{} control regions are integrated with respect to top and \W-tags and therefore have sufficient statistics in spite of the number of b-tags). Except for a normalization factor, the MC is generally consistent with data as function of \met{} for each of the control regions. Therefore, these low \dm{} control region \met{} bins are combined in order to increase the precision of the prediction in these search region bins. A systematic uncertainty on this integration is obtained by comparing the MC \met{} shape to the data shape for the two $N_b=0$ control regions in which no integration is applied. For each control region, the data to MC ratio for each \met{} bin is compared to the fully integrated ratio. The maximum difference, $51\%$ is the systematic uncertainty.

QCD events fail the \qcdhighdm{} or \qcdlowdm{} selection cuts, for the high and low \dm{} regions respectively, and enter the search region due to a leading jet undergoing such severe mis-measurement that it is reconstructed as one of the sub-leading jets. Mis-measurement is parameterized by the jet response, defined as:
\begin{equation}
r_{jet}=\frac{(\pt)_{reco}}{(\pt)_{gen}}.
\end{equation}

A much larger proportion of events in the search region that the control region are in the tail of the $r_{jet}$ distribution, which means that this method relies on the correct modeling of $r_{jet}$ in simulation. Therefore, a $r_{jet}$ correction and uncertainty is extracted from data using the method described in Ref. []. The correciton is derived with the pseudo jet response $(r_{pseudo,jet})$ distribution, applying the QCD control region selection except for the number of jets and b-tagging requirements. We divide this region into two, in the case of the jet aligned to \met{} passing the medium b-tag requirement and in the case of it not passing the light b-tag requirement. The corrections range between $0.81\pm0.12$ and $0.83\pm0.08$ in the case of jets originating from b-quarks and between $1.04\pm0.04$ and $1.82\pm0.15$ for all other jets as can be seen in Table .

The statistical uncertainty on $TF_{QCD}$ is reduced by increasing the effective luminosity of the QCD multijet sample with a method referring to as "local smearing." The method relies on the parametrization of $r_{jet}$, which is only dependent on jet properties.

\subsubsection{QCD Local Smearing}\label{Smearing}

Write smearing method here.

\subsubsection{QCD Corrections}

Figures ? and ? show the \met{} distribution in data and simulation in the QCD control regions for the high \dm{} and lowd \dm{} selections, respectively. The stacked plot labeled "Non-QCD bkg" is the distribution of the non-QCD events as predicted with the estimation methods detailed in this note. The other stacked plot, labeled "Smeared QCD MC" is the smeared QCD simulation yield after applying the $r_{jet}$ corrections. The other two lines show a combination of the predicted non-QCD SM yields and one of two scenarios. "Without $r_{jet}$ corr" is the smeared QCD simulation without the $r_{jet}$ correction and "With orig. QCD MC" is the standard QCD simulation without this correction. There are three important trends in this figure. The first is the purity of the control regions, which gives confidence in using them to predict QCD yields. The second is that the "Without $r_{jet}$ corr" estimation is nearly equivalent to "With orig. QCD MC" in the regions where there are enough statistics to have a meaningful comparison. This improves our confidence in the use of the smeared QCD simulation. Finally, the $r_{jet}$ correction improves the agreement between data simulation, which is a useful validation of this correction. Due to the nature of the background estimation method applied in the high \dm{} search, control regions are utilized for the prediciton of multiple search regions. 

Tables ? and ? summarize the yields in data, the derived transfer factors, and the resulting QCD predictions for the high \dm{} and low \dm{} search regions respectively. The transfer factors in the high \dm{} region actually account for two levels of extrapolation, i.e., the extrapolation from the control regions to the search regions without the requirement of top and W tags, and the extrapolation in top and W tags in the search regions after correcting the top- and W-tagging efficiencies:
\begin{equation}
TF_{QCD}=TF_{QCD}^{CR-SR}\times TF_{QCD}^{SR-extrap}=\frac{N_{MC}(SR)(N_j,N_b,\met)}{N_{MC}(\qcdcr)(N_j,N_b,\met)}\times\frac{N_{MC}(SR)(N_j,N_b,\met,N_t,N_{res},N_W)}{N_{MC}(SR)(N_j,N_b,\met)}
\end{equation}

\subsection{Rare Interactions}
\label{subsec:rare}

The contributions of diboson (WW, WZ, and ZZ) processes are relatively small compared to the other backgrounds, and mainly affect the search regions targeting low \dm{} signal models. The prediciton of the diboson background is obtained directly from simulation, and an uncertainty of 50\% is assigned on the cross section.

The contribution of the ttZ background is also generally very small due to the rarity of this process. However, in search regions that require the presence of more than one top- or W-tagged candidates, this process can constitute a significant fraction of the total SM background due to the strong suppression of the other SM processes. In order to validate the prediction of this background, we define a three-lepton control sample, selected using single-lepton triggers, which requires the presence of exactly three electrons or muons satisfying \pt$>40$ GeV for the leading lepton, \pt$>20$ GeV for the second and third leptons, and no additional lepton with \pt$>10$ GeV. We further require at least five jets, at least two of which are b-tagged. A Z boson mass window of 81-101 GeV is placed on the invariant mass of the same-flavor dilepton \pt{} of this lepton pair is further required to be at least 100 GeV, in order to probe a kinematic region close to the one relevant for the analysis. Figure ? shows the reconstructed Z boson \pt{} distribution observed in this sample. We use the region outside the Z boson mass window (the \met{} distribution in this region is also shown in Fig. ?) to simultaneously constrain the \ttbar background consisting of dilepton \ttbar events with an additional lepton originating from semi-leptonic b-hadron decay or from a misidentified jet, and obtain a scale factor of $1.10\pm0.26$ for the ttZ-like processes in this sample, where the uncertainty is dominated by the statistical uncertainty in the data sample. We therefore apply an uncertainty of 24\% to the normalization of the ttZ background in the analysis. In order to check the extrapolation from the lower Z-\pt{} region of this control sample to the search sample, we evaluate the ttZ scale factor in bins of reconstructed Z boson \pt{} as far as statistics permit. The \pt-binned scale factors are found to be consistent with the inclusive scale factor evaluated for $\pt(Z)>100$ GeV (Fig. sldkjf). Additional experimental and theoretical uncertainties related to PDF and factorization/renormalization scale variations are also assigned. Figure ? shows the number of top- and W-tagged events observed in the ttZ control sample. 
